/*************************************************
*   I.  Static paths & pipeline parameters       *
*************************************************/
base_dir = "/projects/MRC-IEU/research/projects/icep2/wp3/004/working"

if( params.pipeline == 'picard' ) {
    params.phenotype      = "path/to/phenotype.csv"
    params.models         = "${projectDir}/data/models.csv"
    params.data           = "path/to/data"
    params.reads          = "${params.data}/*_R{1,2}*.fastq.gz"
    params.genome_folder  = "${base_dir}/scripts/reference/hg19.fa"
    params.panel          = "${base_dir}/data/blood_cell_types_extended.bed"
    params.outdir         = "${projectDir}/results"
    params.samtools_path  = "/opt/conda/bin/"
    params.panel_qc       = "${projectDir}/data/panel.csv"
}

workDir = "${projectDir}/work"

/*************************************************
*  II.  Detect node resources (use 90% of system)
*************************************************/

// Detect system resources automatically 
def TOTAL_CPUS = Runtime.runtime.availableProcessors()
def TOTAL_MEM_GB = {
    try {
        long kb = new File('/proc/meminfo').readLines().find { it.startsWith('MemTotal') }.tokenize()[1] as long
        (kb / 1024 / 1024) as int
    } catch(e) { 4 }
}()
// Use dynamically %90 CPU, %80 RAM 
def MAX_CPUS = Math.max(1, (TOTAL_CPUS * 0.90) as int)
def MAX_MEM_GB = Math.max(1, (TOTAL_MEM_GB * 0.90) as int)

/*************************************************
*  III.  Resource classes (CPU/MEM â†’ absolute)
*************************************************/

// Define per-job resources for each class (It was calculated by each software user guide)
def CLASS = [
    light : [ cpu: 4 , mem: 8  ],
    medium: [ cpu: 6 , mem: 24 ],
    heavy : [ cpu: 8 , mem: 48 ]
]

// Dynamically calculate maxForks for each class, taking both CPU and RAM into account
def MF = [:]
CLASS.each { k, v ->
    def maxByCpu = (MAX_CPUS / v.cpu) as int
    def maxByMem = (MAX_MEM_GB / v.mem) as int
    // Always pick the lower of CPU- or RAM-limited value; never less than 1
    MF[k] = Math.max(1, Math.min(maxByCpu, maxByMem))
}

/*************************************************
*  IV.  Dynamic ulimit & JVM / pool calculations *
*************************************************/
params.ulimit_stack_mb    = params.ulimit_stack_mb ?: 32768

def ULIMIT_USER = Math.max(4096, (MAX_CPUS * 4) as int)
def ULIMIT_FILE = 65536
def NXF_POOL_THREADS = Math.max(16, MAX_CPUS)

/*************************************************
*  V.  Container paths (override if needed)
*************************************************/
params.wgbs_image = params.wgbs_image ?: "${base_dir}/data/genome-references/containers/wgbs_analysis.sif"
params.meth_image = params.meth_image ?: "${base_dir}/data/genome-references/containers/meth_analysis.sif"
params.qc_image   = params.qc_image   ?: "${base_dir}/data/genome-references/containers/qc_container.sif"

/*************************************************
*  VI.  Executor limits
*************************************************/
executor {
    name   = 'local'
    cpus   = MAX_CPUS
    memory = "${MAX_MEM_GB} GB"
    queueSize = 1000
    pollInterval = '10 sec'
    queueStatInterval = '10 sec'
}

/*************************************************
*  VII.  Process-level env & ulimit
*************************************************/
process {

    beforeScript = """
        ulimit -u 16384
        ulimit -n 4096
        ulimit -s 32768
    """

    env.OPENBLAS_NUM_THREADS = { task.cpus.toString() }
    env.MKL_NUM_THREADS      = { task.cpus.toString() }

    withName:/Fastqc|Interval_file|bedGraph|Processed_bedGraph|Samtools_stats|Multiqc/ {
        cpus      = CLASS.light.cpu
        memory    = "${CLASS.light.mem} GB"
        maxForks  = MF.light
        container = "file://${params.wgbs_image}"
    }
    withName:/QC_Report/ {
        cpus      = CLASS.light.cpu
        memory    = "${CLASS.light.mem} GB"
        maxForks  = MF.light
        container = "file://${params.qc_image}"
    }
    withName:/Trim_galore|Collect_HS_Metrics|Collect_MM_Metrics|MethylKit|MethylDackel|CAMDA|Sambamba/ {
        cpus      = CLASS.medium.cpu
        memory    = "${CLASS.medium.mem} GB"
        maxForks  = MF.medium
        container = "file://${params.wgbs_image}"
    }
    withName:/DNA_Methylation_Scores|Illumina_Matrix/ {
        cpus      = CLASS.medium.cpu
        memory    = "${CLASS.medium.mem} GB"
        maxForks  = MF.medium
        container = "file://${params.meth_image}"
    }
    withName:/Alignment|Mark_duplicated|BSmap_Aligment/ {
        cpus      = CLASS.heavy.cpu
        memory    = "${CLASS.heavy.mem} GB"
        maxForks  = MF.heavy
        container = "file://${params.wgbs_image}"
    }
    withName:/Estimate_cell_counts|DNAm_Matrix|Association_test|Camda_matrix/ {
        cpus      = CLASS.heavy.cpu
        memory    = "${CLASS.heavy.mem} GB"
        maxForks  = MF.heavy
        container = "file://${params.meth_image}"
    }
}

/*************************************************
*  VIII.  Nextflow JVM & thread-pool env
*************************************************/
env {
    NXF_JVM_ARGS               = "-Xms512g -Xmx1024g"
    NXF_ENABLE_VIRTUAL_THREADS = 'true'
    NXF_OPTS                   = "-Dnxf.pool.type=sync -Dnxf.pool.maxThreads=${NXF_POOL_THREADS}"
}

singularity {
    enabled = true
    autoMounts = true
    runOptions = "-B /projects:/projects -B \${HOME}:\${HOME} -B \${baseDir}:\${baseDir} -B \${SINGULARITY_TMPDIR}:/tmp"
    overlay = '--writable'
    env = [
        'OPENBLAS_NUM_THREADS': MAX_CPUS.toString(),
        'MKL_NUM_THREADS'     : MAX_CPUS.toString()
    ]
}

/*************************************************
*   I.  Static paths & pipeline parameters       *
*************************************************/
base_dir = "/projects/MRC-IEU/research/projects/icep2/wp3/004/working"

params 
  {
    phenotype      = "${base_dir}/data/ag24712/data/testdata/phenotype-test-epic.csv"
    models         = "${base_dir}/data/ag24712/data/testdata/models-test-epic.csv"
    data           = "${base_dir}/data/ag24712/data/testdata"
    params.reads   = "${params.data}/*_{R,}{1,2}*.fastq.gz"
    genome_folder  = "${base_dir}/scripts/reference/hg19.fa"
    panel          = "${projectDir}/data/blood_cell_types_extended.bed"
    outdir         = "${projectDir}/results"
    samtools_path  = "/opt/conda/bin/"
    panel_qc       = "${projectDir}/input/panel.csv"
  }

workDir = "${projectDir}/work"

/*************************************************
*  II.  Detect node resources (use 90% of system)
*************************************************/

// Detect system resources automatically 
def TOTAL_CPUS = Runtime.runtime.availableProcessors()
def TOTAL_MEM_GB = {
    try {
        long kb = new File('/proc/meminfo').readLines().find { it.startsWith('MemTotal') }.tokenize()[1] as long
        (kb / 1024 / 1024) as int
    } catch(e) { 4 }
}()
// Use dynamically %90 CPU, %90 RAM 
def MAX_CPUS = Math.max(1, (TOTAL_CPUS * 0.90) as int)
def MAX_MEM_GB = Math.max(1, (TOTAL_MEM_GB * 0.90) as int)

/*************************************************
*  III.  Resource classes (CPU/MEM â†’ absolute)
*************************************************/

// Define per-job resources for each class (It was calculated by each software user guide)
def CLASS = [
    light : [ cpu: 4 , mem: 12 ],
    medium: [ cpu: 6 , mem: 24 ],
    heavy : [ cpu: 8 , mem: 48 ]
]

// Dynamically calculate maxForks for each class, taking both CPU and RAM into account
def MF = [:]
CLASS.each { k, v ->
    def maxByCpu = (MAX_CPUS / v.cpu) as int
    def maxByMem = (MAX_MEM_GB / v.mem) as int
    // Always pick the lower of CPU- or RAM-limited value; never less than 1
    MF[k] = Math.max(1, Math.min(maxByCpu, maxByMem))
}

/*************************************************
*  IV.  Dynamic ulimit & JVM / pool calculations *
*************************************************/
params.ulimit_stack_mb    = params.ulimit_stack_mb ?: 32768

def ULIMIT_USER = Math.max(4096, (MAX_CPUS * 4) as int)
def ULIMIT_FILE = 65536
def NXF_POOL_THREADS = Math.max(16, MAX_CPUS)

/*************************************************
*  V.  Container paths (override if needed)
*************************************************/
params.wgbs_image = 'oras://docker.io/onuroztornaci/methylhead-pipeline:wgbs_analysis'
params.meth_image = 'oras://docker.io/onuroztornaci/methylhead-pipeline:meth_analysis'
params.qc_image   = 'oras://docker.io/onuroztornaci/methylhead-pipeline:qc_container'

/*************************************************
*  VI.  Executor limits
*************************************************/
executor {
    name   = 'local'
    cpus   = MAX_CPUS
    memory = "${MAX_MEM_GB} GB"
    queueSize = 1000
    pollInterval = '10 sec'
    queueStatInterval = '10 sec'
}

/*************************************************
*  VII.  Process-level env & ulimit
*************************************************/
process {

    beforeScript = """
        ulimit -u 16384
        ulimit -n 4096
        ulimit -s 32768
    """

    env.OPENBLAS_NUM_THREADS = { task.cpus.toString() }
    env.MKL_NUM_THREADS      = { task.cpus.toString() }

    withName:/fastqc|interval_file|bedgraph|processed_bedgraph|samtools_stats|multiqc/ {
        cpus      = CLASS.light.cpu
        memory    = "${CLASS.light.mem} GB"
        maxForks  = MF.light
        container = params.wgbs_image
    }
    withName:/qc_report/ {
        cpus      = CLASS.light.cpu
        memory    = "${CLASS.light.mem} GB"
        maxForks  = MF.light
        container = params.qc_image
    }
    withName:/trim_galore|collect_hs_metrics|collect_mm_metrics|methylkit|methyldackel|camda|sambamba/ {
        cpus      = CLASS.medium.cpu
        memory    = "${CLASS.medium.mem} GB"
        maxForks  = MF.medium
        container = params.wgbs_image
    }
    withName:/dna_methylation_scores|illumina_matrix_450k/ {
        cpus      = CLASS.medium.cpu
        memory    = "${CLASS.medium.mem} GB"
        maxForks  = MF.medium
        container = params.meth_image
    }
    withName:/alignment|mark_duplicated|bsmap_aligment/ {
        cpus      = CLASS.heavy.cpu
        memory    = "${CLASS.heavy.mem} GB"
        maxForks  = MF.heavy
        container = params.wgbs_image
    }
    withName:/estimate_cell_counts|methylation_matrix|association_test|camda_matrix/ {
        cpus      = CLASS.heavy.cpu
        memory    = "${CLASS.heavy.mem} GB"
        maxForks  = MF.heavy
        container = params.meth_image
    }
}

/*************************************************
*  VIII.  Nextflow JVM & thread-pool env
*************************************************/
env {
    NXF_JVM_ARGS               = "-Xms4g -Xmx8g"
    NXF_ENABLE_VIRTUAL_THREADS = 'true'
    NXF_OPTS                   = "-Dnxf.pool.type=sync -Dnxf.pool.maxThreads=${NXF_POOL_THREADS}"
}

apptainer {
    enabled = true
    autoMounts = true
    runOptions = "-B \${HOME}:\${HOME} -B \${baseDir}:\${baseDir} -B \${APPTIANER_TMPDIR}:/tmp"
    overlay = '--writable'
    env = [
        'OPENBLAS_NUM_THREADS': MAX_CPUS.toString(),
        'MKL_NUM_THREADS'     : MAX_CPUS.toString()
    ]
}

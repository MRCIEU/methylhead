/*
Config file and Parallelization

queue: 
    Should be arranged according to HPC cluster features. 

clusterOptions: 
    Should be arranged according to HPC cluster features. 
    This command includes additional options for the Slurm job submission. 
    You should configure these options based on the requirements and available resources of your HPC cluster.

scratch: 
    true or false based on your needs. 
    This command indicates whether to use temporary scratch space on the compute nodes for storing intermediate files during job execution.
    - true: Use local scratch space, which can improve I/O performance.
    - false: Use the default shared work directory.

maxForks: 
    This command sets the maximum number of parallel processes that can be run concurrently.
    This parameter sets the maximum number of parallel processes that Nextflow can run at the same time.

queueSize: 
    This command sets the maximum number of jobs that can be submitted to the queue at the same time.
    This parameter defines the maximum number of jobs that Nextflow will submit to the queue simultaneously.

## If the pipeline will be using on login node, those lines should be removed
    executor = 'slurm'
    queue = 'test'
    clusterOptions = '--mem= --account= --ntasks-per-node= --job-name=' 
    scratch = true
    maxForks = 100
    queueSize = 100
*/


if (params.pipeline == 'picard') 
{

params.data=""
params.reads = "${params.data}/*_R{1,2}*.fastq.gz"
params.genome_folder = ""
params.panel = ""
params.outdir = 'Picard_Results'
} 

else if (params.pipeline == 'bismark') 

{
params.data=""
params.reads = "${params.data}/*_R{1,2}*.fastq.gz"
params.genome_folder = ""
params.outdir = "Bismark_Results"
params.u_param = "" 
params.t_param = ""
params.memory_param = "" 
params.multicore= ""
params.cores= ""
} 

process {
    executor = 'slurm'
    queue = 'test'
    clusterOptions = '--mem= --account= --ntasks-per-node= --job-name=' 
    scratch = false
    maxForks = 100
    queueSize = 100
    container = 'file:///*local_path*/dnam_cancer_pipeline_latest.sif'
}

singularity {
    enabled = true
    autoMounts = true
    runOptions = '--writable-tmpfs -B ${baseDir}:${baseDir} -B \${SINGULARITY_TMPDIR}:/tmp'
    cacheDir = '${workDir}'
    overlay= '--writable'
    env = [ 'SINGULARTY_BIND' : "${baseDir}" ]   
}
